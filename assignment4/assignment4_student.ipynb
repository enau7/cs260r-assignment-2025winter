{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS260R Assignment4 Imitatioin Learning: Behavior Cloning & Preference Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this assignment, we will focus on imitation learning & preference learning. Hence we will need some static datasets in hand, before start working on this assignment:\n",
    "\n",
    "Please install the following dependencies:\n",
    "\n",
    "```\n",
    "pip install minari[all]\n",
    "pip install gymnasium==1.0.0\n",
    "pip install mujoco==3.2.3\n",
    "pip install torchrl==0.7.0\n",
    "pip install matplotlib\n",
    "pip install tqdm\n",
    "```\n",
    "\n",
    "We tested with python==3.9.21 on Ubuntu22.04, and it should be fine for any python version >= 3.9. \n",
    "\n",
    "If you have problem installing the dependencies, try switch the python version. And if you have problem installing $\\textbf{MuJoCo}$ on MacOS or Windows, please refer to their github page https://github.com/google-deepmind/mujoco, where they provide a step-by-step instruction for building it from source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: minari[all] in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (0.5.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from minari[all]) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from minari[all]) (4.12.2)\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from typer[all]>=0.9.0->minari[all]) (0.15.2)\n",
      "Requirement already satisfied: gymnasium>=0.28.1 in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from minari[all]) (1.0.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from minari[all]) (24.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from gymnasium>=0.28.1->minari[all]) (3.1.1)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from gymnasium>=0.28.1->minari[all]) (0.0.4)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from typer>=0.9.0->typer[all]>=0.9.0->minari[all]) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from typer>=0.9.0->typer[all]>=0.9.0->minari[all]) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from typer>=0.9.0->typer[all]>=0.9.0->minari[all]) (13.9.4)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from minari[all]) (19.0.1)\n",
      "Requirement already satisfied: jax[cpu] in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from minari[all]) (0.5.1)\n",
      "Requirement already satisfied: google-cloud-storage>=2.5.0 in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from minari[all]) (3.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from minari[all]) (4.67.1)\n",
      "Requirement already satisfied: h5py>=3.8.0 in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from minari[all]) (3.13.0)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from minari[all]) (0.29.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from click>=8.0.0->typer>=0.9.0->typer[all]>=0.9.0->minari[all]) (0.4.6)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=2.26.1 in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from google-cloud-storage>=2.5.0->minari[all]) (2.38.0)\n",
      "Requirement already satisfied: google-api-core<3.0.0dev,>=2.15.0 in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from google-cloud-storage>=2.5.0->minari[all]) (2.24.1)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.4.2 in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from google-cloud-storage>=2.5.0->minari[all]) (2.4.2)\n",
      "Requirement already satisfied: google-resumable-media>=2.7.2 in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from google-cloud-storage>=2.5.0->minari[all]) (2.7.2)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from google-cloud-storage>=2.5.0->minari[all]) (2.32.3)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from google-cloud-storage>=2.5.0->minari[all]) (1.6.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from rich>=10.11.0->typer>=0.9.0->typer[all]>=0.9.0->minari[all]) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from rich>=10.11.0->typer>=0.9.0->typer[all]>=0.9.0->minari[all]) (2.19.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from huggingface-hub->minari[all]) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from huggingface-hub->minari[all]) (2025.2.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from huggingface-hub->minari[all]) (6.0.2)\n",
      "Requirement already satisfied: jaxlib<=0.5.1,>=0.5.1 in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from jax[cpu]; extra == \"create\"->minari[all]) (0.5.1)\n",
      "Requirement already satisfied: ml_dtypes>=0.4.0 in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from jax[cpu]; extra == \"create\"->minari[all]) (0.5.1)\n",
      "Requirement already satisfied: opt_einsum in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from jax[cpu]; extra == \"create\"->minari[all]) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.11.1 in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from jax[cpu]; extra == \"create\"->minari[all]) (1.15.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage>=2.5.0->minari[all]) (1.69.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage>=2.5.0->minari[all]) (5.29.3)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage>=2.5.0->minari[all]) (1.26.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage>=2.5.0->minari[all]) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage>=2.5.0->minari[all]) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage>=2.5.0->minari[all]) (4.9)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->typer[all]>=0.9.0->minari[all]) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage>=2.5.0->minari[all]) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage>=2.5.0->minari[all]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage>=2.5.0->minari[all]) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage>=2.5.0->minari[all]) (2024.12.14)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.26.1->google-cloud-storage>=2.5.0->minari[all]) (0.6.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: typer 0.15.2 does not provide the extra 'all'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium==1.0.0 in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from gymnasium==1.0.0) (2.2.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from gymnasium==1.0.0) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from gymnasium==1.0.0) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from gymnasium==1.0.0) (0.0.4)\n",
      "Requirement already satisfied: mujoco==3.2.3 in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (3.2.3)\n",
      "Requirement already satisfied: absl-py in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from mujoco==3.2.3) (2.1.0)\n",
      "Requirement already satisfied: etils[epath] in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from mujoco==3.2.3) (1.12.0)\n",
      "Requirement already satisfied: glfw in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from mujoco==3.2.3) (2.8.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from mujoco==3.2.3) (2.2.1)\n",
      "Requirement already satisfied: pyopengl in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from mujoco==3.2.3) (3.1.9)\n",
      "Requirement already satisfied: fsspec in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from etils[epath]->mujoco==3.2.3) (2025.2.0)\n",
      "Requirement already satisfied: importlib_resources in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from etils[epath]->mujoco==3.2.3) (6.5.2)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from etils[epath]->mujoco==3.2.3) (4.12.2)\n",
      "Requirement already satisfied: zipp in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from etils[epath]->mujoco==3.2.3) (3.21.0)\n",
      "Requirement already satisfied: torchrl==0.7.0 in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (0.7.0)\n",
      "Requirement already satisfied: torch>=2.6.0 in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from torchrl==0.7.0) (2.6.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from torchrl==0.7.0) (2.2.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from torchrl==0.7.0) (24.2)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from torchrl==0.7.0) (3.1.1)\n",
      "Requirement already satisfied: tensordict>=0.7.0 in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from torchrl==0.7.0) (0.7.2)\n",
      "Requirement already satisfied: orjson in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from tensordict>=0.7.0->torchrl==0.7.0) (3.10.15)\n",
      "Requirement already satisfied: filelock in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from torch>=2.6.0->torchrl==0.7.0) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from torch>=2.6.0->torchrl==0.7.0) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from torch>=2.6.0->torchrl==0.7.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from torch>=2.6.0->torchrl==0.7.0) (3.1.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from torch>=2.6.0->torchrl==0.7.0) (2025.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from torch>=2.6.0->torchrl==0.7.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from sympy==1.13.1->torch>=2.6.0->torchrl==0.7.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from jinja2->torch>=2.6.0->torchrl==0.7.0) (3.0.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (3.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from matplotlib) (4.55.6)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from matplotlib) (2.2.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: imageio in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (2.37.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from imageio) (2.2.1)\n",
      "Requirement already satisfied: pillow>=8.3.2 in c:\\users\\colton\\anaconda3\\envs\\cs260r\\lib\\site-packages (from imageio) (11.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install minari[all]\n",
    "!pip install gymnasium==1.0.0\n",
    "!pip install mujoco==3.2.3\n",
    "!pip install torchrl==0.7.0\n",
    "!pip install matplotlib\n",
    "!pip install tqdm\n",
    "!pip install imageio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Behavior Cloning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 1.1 Prepare Dataset (1pt)\n",
    "\n",
    "In this section, we will need to construct a dataset using Minari and TorchRL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Dataset mujoco/hopper/expert-v0 not found locally at C:\\Users\\Colton\\.minari\\datasets\\mujoco\\hopper\\expert-v0. Use download=True to download the dataset.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Load the dataset from the minari dataset to recover the properties of the dataset\u001b[39;00m\n\u001b[0;32m      9\u001b[0m dataset_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmujoco/hopper/expert-v0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 10\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mminari\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# recover the dimension of state and action space\u001b[39;00m\n\u001b[0;32m     13\u001b[0m state_dim \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Colton\\anaconda3\\envs\\cs260r\\Lib\\site-packages\\minari\\storage\\local.py:51\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(dataset_id, download)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(data_path):\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m download:\n\u001b[1;32m---> 51\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[0;32m     52\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found locally at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Use download=True to download the dataset.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     53\u001b[0m         )\n\u001b[0;32m     55\u001b[0m     hosting\u001b[38;5;241m.\u001b[39mdownload_dataset(dataset_id)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m MinariDataset(data_path)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Dataset mujoco/hopper/expert-v0 not found locally at C:\\Users\\Colton\\.minari\\datasets\\mujoco\\hopper\\expert-v0. Use download=True to download the dataset."
     ]
    }
   ],
   "source": [
    "import minari\n",
    "import gymnasium as gym\n",
    "from torchrl.data.datasets.minari_data import MinariExperienceReplay\n",
    "from torchrl.data.replay_buffers import RandomSampler\n",
    "\n",
    "from typing import List, Tuple, Union\n",
    "\n",
    "# Load the dataset from the minari dataset to recover the properties of the dataset\n",
    "dataset_name = \"mujoco/hopper/expert-v0\"\n",
    "dataset = minari.load_dataset(dataset_name)\n",
    "\n",
    "# recover the dimension of state and action space\n",
    "state_dim = dataset.observation_space.shape[0]\n",
    "action_dim = dataset.action_space.shape[0]\n",
    "\n",
    "# recover the gymnasium environment\n",
    "env = dataset.recover_environment()\n",
    "\n",
    "# setup the parameters for the replay buffer\n",
    "batch_size = 64\n",
    "split_trajs = False\n",
    "\n",
    "\"\"\"\n",
    "Create a torchRL replay buffer from the minari dataset\n",
    "You can use whatever sampler you want, but make sure it will work for the later sections\n",
    "Please refer to the documentation: https://pytorch.org/rl/0.7/reference/generated/torchrl.data.datasets.MinariExperienceReplay.html?highlight=minari#torchrl.data.datasets.MinariExperienceReplay\n",
    "\"\"\"\n",
    "######### Your code here #########\n",
    "replay_buffer = MinariExperienceReplay(dataset_name, 50, download = False)\n",
    "##################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Implement Policies (2pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(-inf, inf, (11,), float64)\n",
      "Box(-1.0, 1.0, (3,), float32)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Print out the state and action space below. \n",
    "Notice that the action space is actually bounded, which means our policy will also need to output \n",
    "\"\"\"\n",
    "print(dataset.observation_space)\n",
    "print(dataset.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# set the seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "\n",
    "# initialization function\n",
    "def weights_init_(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.orthogonal_(m.weight, gain=1)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Create a policy class for continuous actions as you did in Assignment 3\n",
    "The policy should take in a state, and output a vector as the action.\n",
    "Additionally, as we observed above, the action space is bounded between -1 and 1, so make sure use Tanh to restrict the output to be in this scale.\n",
    "\"\"\"\n",
    "class Policy(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim: int,\n",
    "        action_dim: int,\n",
    "        activation: nn.Module=nn.ReLU,\n",
    "        hidden_dim: int=64,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        ###### Your code here #######\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.activation = activation()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.process = {\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            self.activation,\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            self.activation,\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "            nn.Tanh()\n",
    "        }\n",
    "        \n",
    "        #############################\n",
    "        self.apply(weights_init_)\n",
    "    \n",
    "    def forward(self, state: torch.Tensor):\n",
    "        a = state\n",
    "\n",
    "        for func in self.process:\n",
    "            a = func(a)\n",
    "            \n",
    "        return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Implement Trainer (6pt)\n",
    "\n",
    "In this subsection, we need to build our behavior cloning trainer to train the policy. The behavior cloning loss is simply defined as the squared error between the predicted action and the expert action given the state:\n",
    "$$\n",
    "L_{BC}(D^{expert}, \\theta) = \\mathbb{E}_{(s, a) \\sim D^{expert}} \\left[ \\pi_{\\theta}(s) - a\\right]\n",
    "$$\n",
    "And we will train the policy by minimizing this loss using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following evaluation function will be helpful for you to monitor the training process and debug the code\n",
    "@torch.no_grad()\n",
    "def evaluate_policy(policy, env, num_episodes=5):\n",
    "    policy.eval()\n",
    "    rewards = []\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset(seed=episode+1234)\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            state = torch.tensor(state, dtype=torch.float32).to('cuda')\n",
    "            action = policy(state)\n",
    "            action = action.cpu().numpy()\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            done = terminated or truncated\n",
    "            # env.render()\n",
    "            state = next_state\n",
    "        rewards.append(total_reward)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        replay_buffer: MinariExperienceReplay,\n",
    "        env: gym.Env,\n",
    "        lr: float = 1e-4,\n",
    "        device: str = 'cpu',\n",
    "    ):\n",
    "        # Feel free to modify this __init__ function as you needed\n",
    "        self.model = model\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.env = env\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.device = device\n",
    "\n",
    "        self.model.to(self.device)\n",
    "    \n",
    "    def train_step(\n",
    "        self,\n",
    "        states: torch.Tensor,\n",
    "        actions: torch.Tensor\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Finish this function for train self.model for one step\n",
    "        * The loss should be a behavior cloning loss, i.e. MSE loss between the predicted action and the actual expert action\n",
    "        * Apply one step of gradient descent to minimize the loss\n",
    "        * The loss.item() should be returned after the gradient descent is done. \n",
    "        \"\"\"\n",
    "        \n",
    "        ###### Your code here \n",
    "        predictions = self.model(states.to(torch.float32))\n",
    "        loss = F.mse_loss(predictions, actions)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "        ######\n",
    "        \n",
    "    def train(self, num_steps: int, batch_size: int, eval_freq: int) -> Tuple[List[float], List[float]]:\n",
    "        \"\"\"\n",
    "        Finish this function for training the model for num_steps steps\n",
    "        * You should sample [batch_size] data from the replay buffer\n",
    "        * You should print out the loss every [eval_freq] steps\n",
    "        \"\"\"\n",
    "        loss_log: List[float] = []\n",
    "        eval_log: List[float] = []\n",
    "        for step in tqdm(range(num_steps)):\n",
    "            \n",
    "            # Sample some data from the replay buffer\n",
    "            ##### Your code here #####\n",
    "            batch = self.replay_buffer.sample(batch_size)\n",
    "            states = batch['observations'].to(self.device)\n",
    "            actions = batch['actions'].to(self.device)\n",
    "            ############################\n",
    "            \n",
    "            loss = self.train_step(states, actions)\n",
    "            \n",
    "            if eval_freq > 0 and step % eval_freq == 0:\n",
    "                # Do the evaluation, and append the loss as well as the evaluation to the logging list\n",
    "                ##### Your code here #####\n",
    "                eval = evaluate_policy(self.model, self.env)\n",
    "                loss_log.append(loss)\n",
    "                eval_log.append(eval)\n",
    "                print(f\"Step: {step}, Loss: {loss}, Eval: {eval}\")\n",
    "                ############################\n",
    "        \n",
    "        return loss_log, eval_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "optimizer got an empty parameter list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 16\u001b[0m\n\u001b[0;32m      7\u001b[0m lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-3\u001b[39m\n\u001b[0;32m      9\u001b[0m bc_model \u001b[38;5;241m=\u001b[39m  Policy(\n\u001b[0;32m     10\u001b[0m     state_dim\u001b[38;5;241m=\u001b[39mstate_dim, \n\u001b[0;32m     11\u001b[0m     action_dim\u001b[38;5;241m=\u001b[39maction_dim, \n\u001b[0;32m     12\u001b[0m     hidden_dim\u001b[38;5;241m=\u001b[39mhidden_dim, \n\u001b[0;32m     13\u001b[0m     activation\u001b[38;5;241m=\u001b[39mactivation\n\u001b[0;32m     14\u001b[0m )\n\u001b[1;32m---> 16\u001b[0m bc_trainer \u001b[38;5;241m=\u001b[39m \u001b[43mBCTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbc_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m bc_loss_log, bc_eval_log \u001b[38;5;241m=\u001b[39m bc_trainer\u001b[38;5;241m.\u001b[39mtrain(\n\u001b[0;32m     24\u001b[0m     num_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100000\u001b[39m, \n\u001b[0;32m     25\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, \n\u001b[0;32m     26\u001b[0m     eval_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m\n\u001b[0;32m     27\u001b[0m )\n",
      "Cell \u001b[1;32mIn[23], line 14\u001b[0m, in \u001b[0;36mBCTrainer.__init__\u001b[1;34m(self, model, replay_buffer, env, lr, device)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer \u001b[38;5;241m=\u001b[39m replay_buffer\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;241m=\u001b[39m env\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m device\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\Colton\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\optim\\adam.py:99\u001b[0m, in \u001b[0;36mAdam.__init__\u001b[1;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused)\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensor betas[1] must be 1-element\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     87\u001b[0m defaults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m     88\u001b[0m     lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[0;32m     89\u001b[0m     betas\u001b[38;5;241m=\u001b[39mbetas,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     97\u001b[0m     fused\u001b[38;5;241m=\u001b[39mfused,\n\u001b[0;32m     98\u001b[0m )\n\u001b[1;32m---> 99\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefaults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fused:\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m differentiable:\n",
      "File \u001b[1;32mc:\\Users\\Colton\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\optim\\optimizer.py:372\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[1;34m(self, params, defaults)\u001b[0m\n\u001b[0;32m    370\u001b[0m param_groups \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(params)\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(param_groups) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 372\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer got an empty parameter list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(param_groups[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    374\u001b[0m     param_groups \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m: param_groups}]\n",
      "\u001b[1;31mValueError\u001b[0m: optimizer got an empty parameter list"
     ]
    }
   ],
   "source": [
    "# NOTE: Declare the device here\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Feel free to change the hyperparameters\n",
    "hidden_dim = 64\n",
    "activation = nn.ReLU\n",
    "lr = 1e-3\n",
    "\n",
    "bc_model =  Policy(\n",
    "    state_dim=state_dim, \n",
    "    action_dim=action_dim, \n",
    "    hidden_dim=hidden_dim, \n",
    "    activation=activation\n",
    ")\n",
    "\n",
    "bc_trainer = BCTrainer(\n",
    "    model=bc_model, \n",
    "    replay_buffer=replay_buffer,\n",
    "    env=env,\n",
    "    lr=lr, \n",
    "    device=device\n",
    ")\n",
    "bc_loss_log, bc_eval_log = bc_trainer.train(\n",
    "    num_steps=100000, \n",
    "    batch_size=64, \n",
    "    eval_freq=10000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot your results (1pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss and evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Preference Learning\n",
    "In this section, we will implement a preference learning algorithm. A preference learning algorithm learns not only to maximize the probability of outputing expert actions, it also learns to minimize the probability of outputing bad actions. We will leverage a \"negative dataset\", which contains poor-performing trajectories. The algorithm that you will be implementing below is largely based on https://arxiv.org/pdf/2310.13639"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Construct segment dataset (1pt)\n",
    "In this section, we will use segment datasets. The difference between a segment replay buffer and a regular replay buffer is that the segment replay buffer will sample continuous segments, whereas a regular one samples individual [state, action] pairs. Please refer to https://pytorch.org/rl/0.7/reference/generated/torchrl.data.replay_buffers.SliceSampler.html?highlight=slicesampler#torchrl.data.replay_buffers.SliceSampler for details.\n",
    "\n",
    "We need two datasets to perform preference learning: one positive dataset (pos_segment_replay_buffer) and a negative dataset (neg_segment_replay_buffer). The first one contains expert trajectories (and you will have to use proper sampler to slice them into segments), and the latter contains suboptimal trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.data.replay_buffers import SliceSampler\n",
    "\n",
    "pos_dataset_name = \"mujoco/hopper/expert-v0\" # The minari dataset contains expert trajectories\n",
    "neg_dataset_name = \"mujoco/hopper/simple-v0\" # The minari dataset contains low-reward trajecotires\n",
    "\n",
    "\"\"\"\n",
    "Construct two segment dataset below: pos_segment_replay_buffer, neg_segment_replay_buffer\n",
    "\"\"\"\n",
    "batch_size = 64\n",
    "num_segments = 8\n",
    "segment_length: int = batch_size / num_segments\n",
    "traj_key = \"episode\"\n",
    "strict_length = True\n",
    "\n",
    "##### Your code here #####\n",
    "pos_segment_replay_buffer = None\n",
    "neg_segment_replay_buffer = None\n",
    "####################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling function\n",
    "The following function is a helper for sampling segements in your trainer, it does two things:\n",
    "1. Reshape the sampled batch data into segments of data using split_trajectories\n",
    "2. The segment sampler (SliceSampler) in torchRL sometimes gives you longer segments than you would expect. This is due to the lengths of the trajectories are not always multiple of the segment lengths. Hence, we need to make sure the sampled batch is in the right shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.collectors.utils import split_trajectories\n",
    "\n",
    "def sample_segment(slice_replay_buffer, num_segments, segment_length):\n",
    "    batch = None\n",
    "    while True:\n",
    "        batch_size = num_segments * segment_length\n",
    "        batch = slice_replay_buffer.sample(batch_size)\n",
    "        batch = split_trajectories(batch, trajectory_key=\"episode\")\n",
    "        \n",
    "        # make sure there is correct number of segements\n",
    "        if batch.shape[0] != num_segments:\n",
    "            continue\n",
    "        \n",
    "        # make sure the length of segments is correct\n",
    "        if batch.shape[1] != segment_length:\n",
    "            continue\n",
    "        \n",
    "        # make sure there is no padding in the segments\n",
    "        # NOTE: You can use masks to deal with the padding, but you will also need to adjust your learning rate\n",
    "        if batch['mask'].sum() == batch_size:\n",
    "            break\n",
    "        \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Implement Preference Learning Trainer (13pt)\n",
    "\n",
    "In this subsection, your task is to implement the preference learning trainer.\n",
    "\n",
    "Recall our behavior cloning loss is given by:\n",
    "$$\n",
    "L_{BC}(D^{expert}, \\theta) = \\mathbb{E}_{(s, a) \\sim D^{expert}} \\left[ \\pi(s; \\theta) - a\\right]\n",
    "$$\n",
    "\n",
    "In preference learning, we use $\\sigma^+$ to denote preferable segments, and $\\sigma^-$ to denote undesired ones, here, each segment can be writen as a seqence of state and actions, i.e. $\\sigma = [s_k, a_k, s_{k+1}, a_{k+1}, ...]$. \n",
    "Our preference loss is defined as:\n",
    "$$\n",
    "L_{Pref}(D^{expert}, D^{neg}, \\theta) = \\mathbb{E}_{\\sigma^+ \\sim D^{expert}, \\sigma^- \\sim D^{neg}} \\left[\n",
    "    - log \\frac{\n",
    "        e^{\\alpha \\mathbb{P}[\\sigma^+ | \\pi_{\\theta}]}\n",
    "    }{\n",
    "        e^{\\alpha \\mathbb{P}[\\sigma^+ | \\pi_{\\theta}]} + e^{\\alpha \\lambda \\mathbb{P}[\\sigma^- | \\pi_{\\theta}]}\n",
    "    }\n",
    "  \\right]\n",
    "$$\n",
    "where $\\mathbb{P}[\\sigma | \\pi_{\\theta}]$ denotes a measure of the likelihood of policy $\\pi_{\\theta}$ taking the segment $\\sigma$. And $\\alpha, \\lambda$ are hyperparameters, to control the shape of the loss.\n",
    "\n",
    "Here, we use the negative sum of squared error between the predicted action and the real action as the surrogate of this measure:\n",
    "$$\n",
    "\\mathbb{P}[\\sigma | \\pi_{\\theta}] = - \\sum_{(s_i, a_i) \\in \\sigma} \\Vert \\pi_{\\theta}(s_i) - a_i \\Vert^2\n",
    "$$\n",
    "\n",
    "Don't get confused here, the real actions are not necessarily expert actions, the real actions in $\\sigma^-$ is actually sub-optimal.\n",
    "\n",
    "As you may notice, in this loss, there are multiple of exponential and logarithmic operations, which are not numerical stable. Hence, to prevent overflow, you may need to leverage the properties of logarithm to transform this loss first, then implement it. Otherwise, you are likely to encoutner NaN in your training.\n",
    "\n",
    "To wrap up, the final loss function that you will use will be:\n",
    "\n",
    "$$\n",
    "L_{CPL}(\\theta) = L_{Perf}(D^{expert}, D^{neg}, \\theta) + \\beta L_{BC}(D^{expert}, \\theta)\n",
    "$$\n",
    "\n",
    "where the behavior cloning loss will serve as a regularizer, and $\\beta$ is the hyperparameter for controlling it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastivePreferenceTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        pos_segment_buffer: MinariExperienceReplay,\n",
    "        neg_segment_buffer: MinariExperienceReplay,\n",
    "        env: gym.Env,\n",
    "        lr: float = 1e-4,\n",
    "        entropy_coeff: float = 0.01,\n",
    "        lam: float = 1e-3,\n",
    "        alpha: float = 1.0,\n",
    "        bc_loss_coeff: float = 0.1, # the coeffcient for the behavior cloning loss\n",
    "        device: str = 'cpu',\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.pos_segment_buffer = pos_segment_buffer\n",
    "        self.neg_segment_buffer = neg_segment_buffer\n",
    "        self.env = env\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.entropy_coeff = entropy_coeff\n",
    "        self.lam = lam\n",
    "        self.alpha = alpha\n",
    "        self.bc_loss_coeff = bc_loss_coeff\n",
    "        self.device = device\n",
    "        \n",
    "        self.model.to(self.device)\n",
    "        \n",
    "    def train_step(\n",
    "        self, \n",
    "        states: torch.Tensor, \n",
    "        actions: torch.Tensor, \n",
    "        neg_states: torch.Tensor,\n",
    "        neg_actions: torch.Tensor,\n",
    "    ) -> float:\n",
    "        ##### Your code here #####\n",
    "        pass\n",
    "        ##########################\n",
    "    \n",
    "    def train(\n",
    "        self,\n",
    "        num_segments: int,\n",
    "        segment_length: int,\n",
    "        num_steps: int = 100000,\n",
    "        eval_freq: int = 10000,\n",
    "    ):\n",
    "        loss_log = []\n",
    "        eval_log = []\n",
    "        for step in tqdm(range(num_steps)):\n",
    "            \"\"\"\n",
    "            Implement the sampling of the positive and negative segments\n",
    "            The states, actions, neg_states, neg_actions should be of shape (num_segments, segment_length, state_dim) or (num_segments, segment_length, action_dim)\n",
    "            You can use the helper function sample_segment to sample the segments\n",
    "            But you are welcome to implement your own sampling function and using masks, as long as the resulting states / actions are in the correct shape\n",
    "            \"\"\"\n",
    "            ##### Your code here #####\n",
    "            pass\n",
    "            ##########################\n",
    "            \n",
    "            # Do not modify the following assertion statements\n",
    "            assert states.shape == (num_segments, segment_length, state_dim)\n",
    "            assert actions.shape == (num_segments, segment_length, action_dim)\n",
    "            assert neg_states.shape == (num_segments, segment_length, state_dim)\n",
    "            assert neg_actions.shape == (num_segments, segment_length, action_dim)\n",
    "            \n",
    "            loss = self.train_step(states, actions, neg_states, neg_actions)\n",
    "            \n",
    "            if eval_freq > 0 and step % eval_freq == 0:\n",
    "                # Do the evaluation, and append the loss as well as the evaluation to the logging list\n",
    "                ##### Your code here #####\n",
    "                pass\n",
    "                ############################\n",
    "\n",
    "        return loss_log, eval_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a CPL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpl_model = Policy(state_dim=state_dim, action_dim=action_dim, hidden_dim=64, activation=nn.ReLU)\n",
    "cpl_trainer = ContrastivePreferenceTrainer(\n",
    "    model=cpl_model,\n",
    "    pos_segment_buffer=pos_segment_replay_buffer,\n",
    "    neg_segment_buffer=neg_segment_replay_buffer,\n",
    "    env=env, \n",
    "    lr=1e-4,\n",
    "    lam=1.0,\n",
    "    alpha=1.0,\n",
    "    bc_loss_coeff=1,\n",
    "    device='cuda',\n",
    ")\n",
    "cpl_loss_log, cpl_eval_log = cpl_trainer.train(\n",
    "    num_steps=100000, \n",
    "    num_segments=8,\n",
    "    segment_length=8,\n",
    "    eval_freq=10000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the results (1pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Investigate the effect of the hyperparaters $\\alpha, \\lambda, \\beta$ (Bonus 5pt)\n",
    "Try different choice of $\\alpha, \\lambda, \\beta$, see how does the learning curve changes, and explain why they change in those ways based on your intuition. Use your plot the explain your reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs260r",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
